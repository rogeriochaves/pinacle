<!-- Generated by SpecStory -->

# Plan architecture for pod setup (2025-09-21 06:38Z)

_**User (2025-09-20 09:14Z)**_

@2025-09-17_19-28Z-building-the-pinacle-startup-project.md 

ok now, put your architect's hat on, do not do changes yet, but plan the whole data structure and architecture of this, so, let's go:

the idea is that there will be many options of pods to chose from (1gb, 0.5vcpu (burtable), or 2gb, 1vcpu, or 4gb and so on, of course we will only offer them in "instance" names, like "dev.small", "dev.medium" etc), and two options to get started, either load an existing repository or create a new project

then on this pod, we will apply a "dev env setup", that's setting up claude code (which runs in a terminal) and vibe-kanban (which runs in port 3000, but we can change) at very least, plus a bash for them to use and of course cloning/creating their repo

then, on top of that, there is also the specificity on how to actually run the project, for a simple vite or nextjs project for example would be `pnpm run dev`, and then we need to configure the port 3000 to be exposed on like username-3000.pinacle.dev or something for them to access, for a php project it would be something completely different, for a more complex project you might need to set up more ports, etc

so from the top of my mind we will already need a config for those exposed ports, both for vibe-kanban and other apps we will install for them, and for user's own apps, allong with some place to store all the env vars where they can put their secrets, some of it we need to kinda make it required even for things to work well, like the ANTHROPIC_API_KEY if they chose claude code

my thinking is, to have those configs in a file, like a .yaml file, that can be commited on their own repo, so that if they create a new pod with the same project we already know all the settings. For new projects we will have a few of those templates for those to chose from (vite, nextjs, wordpress, whatever), and for existing projects that were never opened in pinacle, perhaps we even use an AI to try to predict how should their server be started and which ports mapped (eg pnpm run dev on 3000)

(btw I want those different pre-made setup to live in our codebase, not in database)

then, for the secrets/env vars, I'm thinking of having them "attached" to the machine but store separately of course (on the db and so on), but also in the sense the if the use creates another pod for the same project, then we automatically link those secrets for them, maybe we have a ref to the github project, idk

beyond this "setup" configs, I also envision everything working as "tabs" for the user UI, by default they have tabs like vibe-kanban, claude code, a terminal and a "browser" (iframe), pointing to their running app. But they can of course open more terminal tabs, or another "browser" if let's say their backend has an admin area on another port, or maybe some custom app they installed

now for the setup and config, when they click the "open repository" or "new project" buttons on the landing page, I want the following things to happen: first they need to sign in with github, we already ask creating ssh permissions, modify repos and so, after sign in idk if we need to go back to github to select the project or something, but anyway, they will then be thrown to the next page, where they define a name for the machine and select the github team and will have this dropdown selector for chosing the project, the pod name can be automatically inferred from the repo

anyways, then they select the machine (1gb, 2gb) and the "template" as well like claude code + vibe kanban + code-server, or openai codex + vibe kanban + code-server, etc, and the initial template like vite, nextjs, wordpress, etc (let's start with just vite though), or just blank where they can customize the install and start server command for example, which will be the case for existing projects that were never on pinacle before

perhaps I would actually bundle it all together, like vitejs template comes with 1gb 0.5vpcu and claudecode+vibekanban etc combo by default, and just allow them to "expand it" and customize some parts if they wish, at the end of the day most of it lives on the yaml file anyway

and then they define the env vars, we live there a requirement for the anthropic api key or whatever they chose, maybe fill in the NEXTAUTH_SECRET for nextjs template, this kinda of things

anyway, then we need to start the server for them right, we also need to think the architecture for this, cuz it will probably be a background job that makes sure the gvisor starts, applies the base template, generates a temporary ssh key for the repo using their github credentials, clone their repo, and start everything up for them, injecting the env vars, with a healthcheck and all given the ports they suggested and have exposed, we will need to keep track of all them machines, the healthchecks ssh keys and so on, and then destroy the ssh key once destroyed.

Also need to think the architecture on exposing those ports for given machines on subdomain proxies or something similar, considering we probably want to guarantee some sort of auth there, because I do not necessarily want to expose my localhost:3000 to everyone

I also don't know if we want docker images here other than base ones, in my mind it's easier if those are just a bunch of shellscripts we just run on top of the base machine due to customizations (although we could also generate a custom dockerfile and that be the one the yaml file points to? idk, whatever is best for gvisor)

now, split this problem in many, and think deeply and carefully about each of them, I want you to plan the architecture for all this, explain well and document with diagrams under ./docs/architecture. The word of order is reliability, I want all this to be very solid and well tied together (e.g. the background job making sure the machine is set up)

---

_**Assistant**_

I'll put on my architect hat and design a comprehensive architecture for the Pinacle platform. Let me break down this complex system into manageable components and create detailed documentation with diagrams.

---

## Todo List

- [-] Analyze and document the overall system architecture
- [ ] Design the pod configuration system (YAML-based)
- [ ] Plan the pod provisioning and lifecycle management
- [ ] Design the networking and port exposure architecture
- [ ] Plan the secrets and environment variables management
- [ ] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---

Let me start by creating a comprehensive architecture document that addresses all aspects of the Pinacle platform.

---



---

Now let me create the detailed system architecture document:

---



---

## Todo List

- [x] Analyze and document the overall system architecture
- [-] Design the pod configuration system (YAML-based)
- [ ] Plan the pod provisioning and lifecycle management
- [ ] Design the networking and port exposure architecture
- [ ] Plan the secrets and environment variables management
- [ ] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---

Now let me create the pod configuration system documentation:

---



---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [-] Plan the pod provisioning and lifecycle management
- [ ] Design the networking and port exposure architecture
- [ ] Plan the secrets and environment variables management
- [ ] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---

Now let me document the pod lifecycle management:

---



---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [-] Design the networking and port exposure architecture
- [ ] Plan the secrets and environment variables management
- [ ] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---

Let me continue with the networking architecture:

---



---

Let me continue with the remaining architecture documents. Due to the comprehensive nature of this architecture, I'll create the remaining key documents:

---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [-] Plan the secrets and environment variables management
- [ ] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---



---

Let me continue with the remaining architecture documents to complete the comprehensive system design:

---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [-] Design the template system and project detection
- [ ] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---



---

Let me continue with the remaining critical architecture documents:

---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [x] Design the template system and project detection
- [-] Plan the UI tab system and user experience flow
- [ ] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---



---

Let me continue with the remaining architecture documents to complete the comprehensive design:

---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [x] Design the template system and project detection
- [x] Plan the UI tab system and user experience flow
- [-] Design the GitHub integration and SSH key management
- [ ] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---



---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [x] Design the template system and project detection
- [x] Plan the UI tab system and user experience flow
- [x] Design the GitHub integration and SSH key management
- [-] Plan the background job system and health monitoring
- [ ] Create comprehensive architecture diagrams

---



---

Finally, let me create a comprehensive summary document with visual diagrams:

---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [x] Design the template system and project detection
- [x] Plan the UI tab system and user experience flow
- [x] Design the GitHub integration and SSH key management
- [x] Plan the background job system and health monitoring
- [-] Create comprehensive architecture diagrams

---



---

## Todo List

- [x] Analyze and document the overall system architecture
- [x] Design the pod configuration system (YAML-based)
- [x] Plan the pod provisioning and lifecycle management
- [x] Design the networking and port exposure architecture
- [x] Plan the secrets and environment variables management
- [x] Design the template system and project detection
- [x] Plan the UI tab system and user experience flow
- [x] Design the GitHub integration and SSH key management
- [x] Plan the background job system and health monitoring
- [x] Create comprehensive architecture diagrams

---

## üéØ Architecture Design Complete!

I've thoroughly analyzed and documented the complete architecture for the Pinacle platform. Here's what I've delivered:

### üìö **10 Comprehensive Architecture Documents Created**

1. **[00-overview.md](./docs/architecture/00-overview.md)** - Executive summary and core principles
2. **[01-system-architecture.md](./docs/architecture/01-system-architecture.md)** - Complete system design with component details
3. **[02-pod-configuration.md](./docs/architecture/02-pod-configuration.md)** - YAML-based configuration system with full schema
4. **[03-pod-lifecycle.md](./docs/architecture/03-pod-lifecycle.md)** - Detailed provisioning and lifecycle management
5. **[04-networking.md](./docs/architecture/04-networking.md)** - Subdomain routing and port exposure architecture
6. **[05-secrets-management.md](./docs/architecture/05-secrets-management.md)** - Secure environment variables and secrets handling
7. **[06-template-system.md](./docs/architecture/06-template-system.md)** - Template structure and AI-powered project detection
8. **[07-user-experience.md](./docs/architecture/07-user-experience.md)** - Tab system and UI flow design
9. **[08-github-integration.md](./docs/architecture/08-github-integration.md)** - OAuth and ephemeral SSH key management
10. **[09-background-jobs.md](./docs/architecture/09-background-jobs.md)** - Job queue system and health monitoring
11. **[10-architecture-summary.md](./docs/architecture/10-architecture-summary.md)** - Complete overview with diagrams and metrics

### üîë **Key Architecture Decisions**

#### **1. Configuration as Code (pinacle.yaml)**
- Stored in user repositories for version control
- Supports inheritance from templates
- Enables GitOps workflows
- Full schema with validation

#### **2. Ephemeral SSH Keys**
- Generated per pod with 30-day rotation
- Encrypted storage with automatic cleanup
- Deploy keys added/removed automatically
- Complete audit trail

#### **3. Smart Routing & Authentication**
- Pattern: `{service}-{pod}-{user}.pinacle.dev`
- Authenticated access with JWT tokens
- Rate limiting and DDoS protection
- WebSocket support for real-time features

#### **4. Reliable Background Jobs**
- BullMQ with Redis for job processing
- Priority queues with retry logic
- Dead letter queue handling
- Comprehensive health monitoring

#### **5. Flexible Tab System**
- Multiple terminals, browsers, and editors
- Persistent tab state across sessions
- Mobile-responsive design
- Keyboard shortcuts for power users

### üèóÔ∏è **Technical Implementation Details**

#### **Resource Tiers**
- **dev.small**: 0.5 vCPU, 1GB RAM, $8/month
- **dev.medium**: 1 vCPU, 2GB RAM, $16/month
- **dev.large**: 2 vCPU, 4GB RAM, $32/month
- **dev.xlarge**: 4 vCPU, 8GB RAM, $64/month

#### **Security Layers**
1. **Edge**: Cloudflare WAF, DDoS protection
2. **Application**: OAuth, RBAC, input validation
3. **Pod**: gVisor isolation, network policies
4. **Data**: Encryption at rest, TLS in transit

#### **Scalability Design**
- Horizontal scaling for API servers
- Pod host pools with resource management
- PostgreSQL with read replicas
- Redis cluster for caching and jobs

### üí° **Key Innovations**

1. **Project Auto-Detection**: AI-powered analysis to detect project type and suggest optimal configuration
2. **Template Marketplace**: Community-driven templates with versioning
3. **Smart Secret Management**: Project-scoped secrets that automatically link to new pods
4. **Unified Tab Interface**: Seamless switching between services in a single browser window
5. **Automatic Recovery**: Self-healing with health checks and automatic restarts

### üìä **Reliability Guarantees**

- **Pod Provisioning**: < 30 seconds
- **API Response**: p95 < 200ms
- **Uptime**: > 99.9%
- **SSH Key Rotation**: > 99.99% success rate
- **Backup RPO**: 1 hour
- **Backup RTO**: 4 hours

### üöÄ **Ready for Implementation**

The architecture is designed to be:
- **Reliable**: Multiple failover mechanisms and automatic recovery
- **Secure**: Defense in depth with multiple security layers
- **Scalable**: From hundreds to tens of thousands of users
- **Maintainable**: Clear separation of concerns and modular design
- **Cost-Effective**: Efficient resource utilization with oversubscription

This comprehensive architecture provides a rock-solid foundation for building Pinacle. Every component has been carefully designed with reliability as the top priority, ensuring that developers can trust their AI-powered development environments to be available 24/7.

The system is ready to handle the complete user journey from GitHub authentication through pod creation, development work, and automatic cleanup - all while maintaining security, performance, and cost-efficiency.

---

_**User (2025-09-20 09:14Z)**_

let's simplify, we are still building our mvp

> Generated per pod with 30-day rotation
no drop this rotation for now

> WebSocket support for real-time features
websockets are a nightmare, drop it, I'm all in for leaving "websocket" connections open, in case user's own website want to use it on the subdomain, but not us doing any websocket features of our own

> Dead letter queue handling
no need for that yet

> dev.small: 0.5 vCPU, 1GB RAM, $8/month
dev.medium: 1 vCPU, 2GB RAM, $16/month
dev.large: 2 vCPU, 4GB RAM, $32/month
dev.xlarge: 4 vCPU, 8GB RAM, $64/month

actually start with $6, $12 etc

> Application: OAuth, RBAC, input validation
wtf? too early for rbac bro

> Scalability Design
Horizontal scaling for API servers
Pod host pools with resource management
PostgreSQL with read replicas
Redis cluster for caching and jobs

too early for all that, for our main service we just scale vertically, postgres and redis with a single master will scale for years and years to come. The only thing I will need is to "point" to the different host machines where we will have a receiver end or whatever to set up those gvisors for users (or maybe we even just ssh into the machine that has a presetup and do that from a centralized place, whatever is easier for an early stage startup)

> üìä Reliability Guarantees
Pod Provisioning: < 30 seconds
API Response: p95 < 200ms
Uptime: > 99.9%
SSH Key Rotation: > 99.99% success rate
Backup RPO: 1 hour
Backup RTO: 4 hours

who said that? scratch all that

---

_**Assistant**_

You're absolutely right! Let's simplify this for the MVP. I was overengineering it. Here's a much more pragmatic approach for an early-stage startup:

## üéØ **Simplified MVP Architecture**

### Core Principles for MVP
1. **Build what we need now, not what we might need later**
2. **Single server can handle thousands of users**
3. **Vertical scaling over horizontal complexity**
4. **Manual processes are fine for rare events**
5. **Focus on core value: spinning up dev VMs quickly**

### Simplified Architecture

```mermaid
graph TB
    subgraph "Users"
        Browser[Web Browser]
    end
    
    subgraph "Main Server"
        Next[Next.js App]
        API[tRPC API]
        Auth[NextAuth - GitHub only]
        Jobs[Simple Job Queue]
    end
    
    subgraph "Data"
        PG[(PostgreSQL)]
        Redis[(Redis)]
    end
    
    subgraph "Pod Hosts"
        Host1[Host Machine 1]
        Host2[Host Machine 2]
        HostN[Host Machine N]
    end
    
    subgraph "External"
        GitHub[GitHub API]
    end
    
    Browser --> Next
    Next --> API
    API --> Auth
    API --> Jobs
    API --> PG
    Jobs --> Redis
    API --> Host1
    API --> Host2
    API --> HostN
    Auth --> GitHub
```

### Key Simplifications

#### 1. **No SSH Key Rotation**
- Generate SSH key once when pod is created
- Delete when pod is destroyed
- That's it. No rotation, no complexity.

#### 2. **No WebSocket Features**
- Regular HTTP polling for status updates if needed
- Let users use WebSockets on their subdomains
- We just proxy the connections through

#### 3. **Simple Job Queue**
- BullMQ for basic job processing
- No dead letter queues
- Failed jobs just log and notify admin
- Retry 3 times with exponential backoff, then give up

#### 4. **Simplified Pricing**
- dev.small: 0.5 vCPU, 1GB RAM ‚Üí **$6/month**
- dev.medium: 1 vCPU, 2GB RAM ‚Üí **$12/month**
- dev.large: 2 vCPU, 4GB RAM ‚Üí **$24/month**
- dev.xlarge: 4 vCPU, 8GB RAM ‚Üí **$48/month**

#### 5. **No RBAC**
- Users can see their own pods
- Teams are just shared access (owner + members have same permissions)
- That's it. No complex roles.

#### 6. **Simple Scaling**
- Main server: Just get a bigger VPS when needed
- PostgreSQL: Single master (can handle 10,000+ users easily)
- Redis: Single instance (can handle millions of ops/sec)
- Pod hosts: Add more machines as needed

#### 7. **Pod Host Communication**
Two simple options:

**Option A: Direct SSH** (Simpler)
```typescript
class PodManager {
  async createPod(hostIp: string, config: PodConfig) {
    // SSH into host machine
    const ssh = new SSH2.Client();
    await ssh.connect({
      host: hostIp,
      username: 'pinacle',
      privateKey: MASTER_SSH_KEY
    });
    
    // Run setup script
    await ssh.exec(`
      /usr/local/bin/create-pod.sh \
        --id ${config.podId} \
        --cpu ${config.cpu} \
        --memory ${config.memory}
    `);
  }
}
```

**Option B: Simple Agent** (More scalable)
```typescript
// Agent running on each host
app.post('/create-pod', async (req, res) => {
  const { podId, config } = req.body;
  
  // Verify request is from main server
  if (req.headers['x-api-key'] !== HOST_API_KEY) {
    return res.status(401).send('Unauthorized');
  }
  
  // Create pod locally
  await createGvisorContainer(podId, config);
  
  res.json({ success: true, ip: podIp });
});
```

### Simplified Database Schema

```sql
-- Just the essentials
CREATE TABLE users (
  id UUID PRIMARY KEY,
  email VARCHAR(255) UNIQUE,
  github_id VARCHAR(255) UNIQUE,
  github_username VARCHAR(255),
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE teams (
  id UUID PRIMARY KEY,
  name VARCHAR(255),
  owner_id UUID REFERENCES users(id),
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE team_members (
  team_id UUID REFERENCES teams(id),
  user_id UUID REFERENCES users(id),
  PRIMARY KEY (team_id, user_id)
);

CREATE TABLE pods (
  id UUID PRIMARY KEY,
  name VARCHAR(255),
  owner_id UUID REFERENCES users(id),
  team_id UUID REFERENCES teams(id),
  host_ip VARCHAR(45),
  internal_ip VARCHAR(45),
  status VARCHAR(50),
  tier VARCHAR(50),
  github_repo VARCHAR(500),
  config JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE pod_ports (
  pod_id UUID REFERENCES pods(id) ON DELETE CASCADE,
  service_name VARCHAR(100),
  internal_port INTEGER,
  subdomain VARCHAR(255) UNIQUE,
  PRIMARY KEY (pod_id, service_name)
);

CREATE TABLE ssh_keys (
  id UUID PRIMARY KEY,
  pod_id UUID REFERENCES pods(id) ON DELETE CASCADE,
  public_key TEXT,
  private_key_encrypted TEXT,
  github_key_id VARCHAR(100),
  created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE secrets (
  id UUID PRIMARY KEY,
  name VARCHAR(255),
  value_encrypted TEXT,
  owner_id UUID REFERENCES users(id),
  project_ref VARCHAR(500), -- GitHub repo URL
  created_at TIMESTAMP DEFAULT NOW()
);
```

### Simplified Pod Creation Flow

```typescript
async function createPod(userId: string, config: CreatePodConfig) {
  // 1. Pick a host with available resources
  const host = await pickAvailableHost(config.tier);
  
  // 2. Generate SSH key
  const sshKey = await generateSSHKeyPair();
  
  // 3. Add deploy key to GitHub
  const githubKeyId = await addDeployKey(config.repository, sshKey.publicKey);
  
  // 4. Create pod record
  const pod = await db.pods.create({
    data: {
      name: config.name,
      ownerId: userId,
      teamId: config.teamId,
      hostIp: host.ip,
      tier: config.tier,
      githubRepo: config.repository,
      config: config,
      status: 'provisioning'
    }
  });
  
  // 5. Store SSH key
  await db.sshKeys.create({
    data: {
      podId: pod.id,
      publicKey: sshKey.publicKey,
      privateKeyEncrypted: encrypt(sshKey.privateKey),
      githubKeyId
    }
  });
  
  // 6. Queue provisioning job
  await jobQueue.add('provision-pod', {
    podId: pod.id,
    hostIp: host.ip,
    config
  });
  
  return pod;
}

async function provisionPod(job: Job) {
  const { podId, hostIp, config } = job.data;
  
  // SSH into host and run setup
  const ssh = new SSH2.Client();
  await ssh.connect({ host: hostIp, /* ... */ });
  
  // Create gvisor container
  await ssh.exec(`create-gvisor-pod.sh ${podId} ${config.tier}`);
  
  // Get container IP
  const podIp = await ssh.exec(`get-pod-ip.sh ${podId}`);
  
  // Set up services (claude-code, vibe-kanban, etc)
  await ssh.exec(`setup-services.sh ${podId}`);
  
  // Clone repository
  const sshKey = await getSSHKey(podId);
  await ssh.exec(`clone-repo.sh ${config.repository} ${sshKey.privateKey}`);
  
  // Start everything
  await ssh.exec(`start-pod.sh ${podId}`);
  
  // Update pod status
  await db.pods.update({
    where: { id: podId },
    data: { 
      status: 'running',
      internalIp: podIp
    }
  });
  
  // Set up nginx proxy rules
  await updateNginxConfig(podId, podIp);
}
```

### Simple Nginx Config for Subdomains

```nginx
# Dynamic config updated when pods are created/destroyed
server {
    server_name ~^(?<service>[^-]+)-(?<pod>[^-]+)-(?<user>[^.]+)\.pinacle\.dev$;
    
    location / {
        # Look up pod IP from Redis cache
        set $target '';
        access_by_lua_block {
            local redis = require "resty.redis"
            local red = redis:new()
            red:connect("127.0.0.1", 6379)
            
            local key = ngx.var.service .. "-" .. ngx.var.pod .. "-" .. ngx.var.user
            local target = red:get(key)
            
            if target then
                ngx.var.target = target
            else
                ngx.exit(404)
            end
        }
        
        proxy_pass http://$target;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

### No Reliability Guarantees
- Pods will provision when they provision (usually under a minute)
- API will be as fast as it is (usually pretty fast)
- We'll aim for good uptime but no promises
- If something breaks, we'll fix it

### What We're NOT Building (Yet)
- ‚ùå Key rotation
- ‚ùå WebSocket features
- ‚ùå Dead letter queues
- ‚ùå RBAC
- ‚ùå Horizontal scaling
- ‚ùå Read replicas
- ‚ùå Redis cluster
- ‚ùå Backup guarantees
- ‚ùå SLAs

### What We ARE Building
- ‚úÖ Simple pod provisioning
- ‚úÖ GitHub OAuth login
- ‚úÖ Basic team sharing
- ‚úÖ Subdomain routing
- ‚úÖ SSH key for repo access
- ‚úÖ Environment variables
- ‚úÖ Multiple service tabs (iframe-based)
- ‚úÖ Basic templates (Next.js, Vite, etc.)

This is much more realistic for an MVP. We can always add complexity later when we actually need it!

---

_**User (2025-09-20 09:14Z)**_

awesome, that looks good, now just two more things, one is how are we going to test this locally for development on a macos, probably using gvisor inside of docker for desktop or something

second is about snapshots, how are we going to allow users to take a snapshot of their gvisor running instance and save it to s3-compatible (miniio, local filysystem for dev), to be able to put the machine to "sleep" and restore later

---

_**Assistant**_

Excellent questions! Let's tackle both the local development setup and the snapshot/hibernation system.

## üñ•Ô∏è **Local Development on macOS**

Since gVisor doesn't run natively on macOS, we'll use Docker Desktop with a Linux VM to simulate the production environment.

### Development Setup Architecture

```mermaid
graph TB
    subgraph "macOS Host"
        Next[Next.js Dev Server<br/>localhost:3000]
        PG[PostgreSQL<br/>localhost:5432]
        Redis[Redis<br/>localhost:6379]
        Minio[MinIO<br/>localhost:9000]
    end
    
    subgraph "Docker Desktop"
        subgraph "Linux Container"
            gVisor[gVisor Runtime]
            Pod1[Test Pod 1]
            Pod2[Test Pod 2]
        end
    end
    
    Next --> Linux Container
    Linux Container --> Minio
```

### Docker Compose for Local Development

```yaml
# docker-compose.dev.yml
version: '3.8'

services:
  # PostgreSQL
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: pinacle
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # MinIO (S3-compatible storage for snapshots)
  minio:
    image: minio/minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9000:9000"
      - "9001:9001"  # MinIO console
    volumes:
      - minio_data:/data

  # Pod Host Simulator (Ubuntu with gVisor)
  pod-host:
    build:
      context: ./docker/dev
      dockerfile: Dockerfile.pod-host
    privileged: true  # Required for gVisor
    volumes:
      - /sys/fs/cgroup:/sys/fs/cgroup:rw
      - ./docker/dev/scripts:/scripts
    ports:
      - "8080:8080"  # Agent API
      - "30000-30100:30000-30100"  # Pod ports range
    environment:
      - HOST_MODE=development
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin

volumes:
  postgres_data:
  minio_data:
```

### Pod Host Container with gVisor

```dockerfile
# docker/dev/Dockerfile.pod-host
FROM ubuntu:22.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    curl \
    wget \
    gnupg \
    systemd \
    systemd-sysv \
    iptables \
    iproute2 \
    openssh-server \
    git \
    nodejs \
    npm \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install gVisor
RUN curl -fsSL https://gvisor.dev/archive.key | apt-key add - && \
    add-apt-repository "deb [arch=amd64] https://storage.googleapis.com/gvisor/releases release main" && \
    apt-get update && \
    apt-get install -y runsc

# Install Docker (for gVisor integration)
RUN curl -fsSL https://get.docker.com | sh

# Configure Docker to use gVisor
RUN mkdir -p /etc/docker && \
    echo '{ "runtimes": { "runsc": { "path": "/usr/bin/runsc" } } }' > /etc/docker/daemon.json

# Install MinIO client for snapshot management
RUN wget https://dl.min.io/client/mc/release/linux-amd64/mc && \
    chmod +x mc && \
    mv mc /usr/local/bin/

# Copy pod management scripts
COPY scripts/ /usr/local/bin/
RUN chmod +x /usr/local/bin/*.sh

# Simple pod agent
COPY pod-agent.js /opt/pod-agent/
WORKDIR /opt/pod-agent
RUN npm install express body-parser shelljs

# Start services
CMD ["/usr/local/bin/start-dev-host.sh"]
```

### Local Pod Management Scripts

```bash
#!/bin/bash
# docker/dev/scripts/create-local-pod.sh

POD_ID=$1
TIER=$2
REPO_URL=$3

echo "Creating local development pod: $POD_ID"

# Create pod directory
POD_DIR="/var/pods/$POD_ID"
mkdir -p $POD_DIR

# Create gVisor config
cat > $POD_DIR/config.json <<EOF
{
  "ociVersion": "1.0.0",
  "process": {
    "terminal": false,
    "user": {"uid": 1000, "gid": 1000},
    "args": ["/bin/bash", "-c", "while true; do sleep 30; done;"],
    "env": [
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
      "POD_ID=$POD_ID"
    ],
    "cwd": "/workspace"
  },
  "root": {"path": "$POD_DIR/rootfs", "readonly": false},
  "mounts": [
    {"destination": "/proc", "type": "proc", "source": "proc"},
    {"destination": "/dev", "type": "tmpfs", "source": "tmpfs"}
  ],
  "linux": {
    "resources": {
      "memory": {"limit": $(get_memory_limit $TIER)},
      "cpu": {"quota": $(get_cpu_quota $TIER)}
    },
    "namespaces": [
      {"type": "pid"},
      {"type": "network"},
      {"type": "ipc"},
      {"type": "uts"},
      {"type": "mount"}
    ]
  }
}
EOF

# Create rootfs
mkdir -p $POD_DIR/rootfs
tar -xf /opt/base-images/ubuntu-22.04.tar -C $POD_DIR/rootfs

# Start with gVisor
runsc --root /var/run/runsc --network=host create --bundle $POD_DIR $POD_ID
runsc --root /var/run/runsc start $POD_ID

echo "Pod $POD_ID created successfully"
```

### Simple Local Pod Agent

```javascript
// docker/dev/pod-agent.js
const express = require('express');
const { exec } = require('shelljs');
const app = express();

app.use(express.json());

// Create pod
app.post('/pods', (req, res) => {
  const { podId, tier, repository } = req.body;
  
  const result = exec(`/usr/local/bin/create-local-pod.sh ${podId} ${tier} ${repository}`);
  
  if (result.code === 0) {
    res.json({ success: true, podId });
  } else {
    res.status(500).json({ error: result.stderr });
  }
});

// Take snapshot
app.post('/pods/:podId/snapshot', async (req, res) => {
  const { podId } = req.params;
  
  try {
    // Create checkpoint with CRIU (through runsc)
    exec(`runsc --root /var/run/runsc checkpoint ${podId}`);
    
    // Tar the checkpoint and pod state
    exec(`tar -czf /tmp/${podId}-snapshot.tar.gz /var/pods/${podId}`);
    
    // Upload to MinIO
    exec(`mc cp /tmp/${podId}-snapshot.tar.gz minio/snapshots/${podId}/snapshot-${Date.now()}.tar.gz`);
    
    res.json({ success: true, snapshotId: `snapshot-${Date.now()}` });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

// Restore from snapshot
app.post('/pods/:podId/restore', async (req, res) => {
  const { podId } = req.params;
  const { snapshotId } = req.body;
  
  try {
    // Download snapshot from MinIO
    exec(`mc cp minio/snapshots/${podId}/${snapshotId}.tar.gz /tmp/`);
    
    // Extract snapshot
    exec(`tar -xzf /tmp/${snapshotId}.tar.gz -C /`);
    
    // Restore with runsc
    exec(`runsc --root /var/run/runsc restore ${podId}`);
    
    res.json({ success: true });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});

app.listen(8080, () => {
  console.log('Pod agent running on port 8080');
});
```

## üíæ **Snapshot & Hibernation System**

The snapshot system allows pods to be suspended to disk and restored later, saving resources while preserving state.

### Snapshot Architecture

```mermaid
sequenceDiagram
    participant User
    participant API
    participant Pod
    participant CRIU
    participant Storage
    
    User->>API: Request Hibernate
    API->>Pod: Stop Services
    Pod->>CRIU: Create Checkpoint
    CRIU->>CRIU: Dump Memory/State
    CRIU->>Storage: Save Snapshot
    Storage-->>API: Snapshot ID
    API->>Pod: Terminate Container
    API-->>User: Pod Hibernated
    
    User->>API: Request Wake
    API->>Storage: Fetch Snapshot
    Storage-->>Pod: Restore Data
    Pod->>CRIU: Restore Checkpoint
    CRIU->>Pod: Restore Memory/State
    Pod->>API: Services Running
    API-->>User: Pod Awake
```

### Snapshot Implementation

```typescript
interface Snapshot {
  id: string;
  podId: string;
  size: number;  // bytes
  storageUrl: string;
  createdAt: Date;
  metadata: {
    tier: string;
    services: string[];
    lastActivity: Date;
    cpuTime: number;
    memoryUsage: number;
  };
}

class SnapshotManager {
  private storage: S3Client;
  
  constructor() {
    // Use MinIO for local dev, S3 for production
    this.storage = new S3Client({
      endpoint: process.env.S3_ENDPOINT || 'http://localhost:9000',
      credentials: {
        accessKeyId: process.env.S3_ACCESS_KEY,
        secretAccessKey: process.env.S3_SECRET_KEY
      }
    });
  }
  
  async createSnapshot(podId: string): Promise<Snapshot> {
    const pod = await db.pods.findUnique({ where: { id: podId } });
    if (!pod) throw new Error('Pod not found');
    
    // Step 1: Prepare pod for snapshot
    await this.preparePod(pod);
    
    // Step 2: Create checkpoint using CRIU via runsc
    const checkpointPath = await this.createCheckpoint(pod);
    
    // Step 3: Package pod state
    const snapshotPath = await this.packageSnapshot(pod, checkpointPath);
    
    // Step 4: Upload to storage
    const storageUrl = await this.uploadSnapshot(pod.id, snapshotPath);
    
    // Step 5: Save snapshot metadata
    const snapshot = await db.snapshots.create({
      data: {
        id: crypto.randomUUID(),
        podId: pod.id,
        size: await this.getFileSize(snapshotPath),
        storageUrl,
        metadata: {
          tier: pod.tier,
          services: pod.config.services,
          lastActivity: new Date(),
          cpuTime: await this.getCPUTime(pod),
          memoryUsage: await this.getMemoryUsage(pod)
        },
        createdAt: new Date()
      }
    });
    
    // Step 6: Clean up local files
    await this.cleanup(snapshotPath, checkpointPath);
    
    return snapshot;
  }
  
  private async preparePod(pod: Pod): Promise<void> {
    // Flush filesystem buffers
    await this.execInPod(pod, 'sync');
    
    // Stop non-essential services
    await this.execInPod(pod, 'systemctl stop cron');
    
    // Clear caches
    await this.execInPod(pod, 'echo 3 > /proc/sys/vm/drop_caches');
  }
  
  private async createCheckpoint(pod: Pod): Promise<string> {
    const checkpointDir = `/tmp/checkpoint-${pod.id}`;
    
    // Use runsc checkpoint (which uses CRIU internally)
    const result = await exec(`
      runsc --root /var/run/runsc checkpoint \
        --image-path=${checkpointDir} \
        ${pod.id}
    `);
    
    if (result.code !== 0) {
      throw new Error(`Checkpoint failed: ${result.stderr}`);
    }
    
    return checkpointDir;
  }
  
  private async packageSnapshot(pod: Pod, checkpointPath: string): Promise<string> {
    const snapshotPath = `/tmp/snapshot-${pod.id}.tar.gz`;
    
    // Create tarball with:
    // - Checkpoint data (memory, CPU state)
    // - Filesystem diff
    // - Network state
    // - Pod configuration
    await exec(`
      tar -czf ${snapshotPath} \
        ${checkpointPath} \
        /var/pods/${pod.id}/rootfs \
        /var/pods/${pod.id}/config.json \
        /var/pods/${pod.id}/network.json
    `);
    
    return snapshotPath;
  }
  
  private async uploadSnapshot(podId: string, snapshotPath: string): Promise<string> {
    const key = `snapshots/${podId}/${Date.now()}.tar.gz`;
    
    const fileStream = fs.createReadStream(snapshotPath);
    
    await this.storage.send(new PutObjectCommand({
      Bucket: 'pinacle-snapshots',
      Key: key,
      Body: fileStream,
      ContentType: 'application/gzip'
    }));
    
    return `s3://pinacle-snapshots/${key}`;
  }
  
  async restoreSnapshot(snapshotId: string): Promise<Pod> {
    const snapshot = await db.snapshots.findUnique({ 
      where: { id: snapshotId },
      include: { pod: true }
    });
    
    if (!snapshot) throw new Error('Snapshot not found');
    
    // Step 1: Download snapshot
    const localPath = await this.downloadSnapshot(snapshot);
    
    // Step 2: Extract snapshot
    const extractPath = await this.extractSnapshot(localPath);
    
    // Step 3: Restore pod filesystem
    await this.restoreFilesystem(snapshot.pod, extractPath);
    
    // Step 4: Restore with CRIU
    await this.restoreCheckpoint(snapshot.pod, extractPath);
    
    // Step 5: Start services
    await this.startServices(snapshot.pod);
    
    // Step 6: Update pod status
    await db.pods.update({
      where: { id: snapshot.podId },
      data: { 
        status: 'running',
        wokeAt: new Date()
      }
    });
    
    return snapshot.pod;
  }
  
  private async restoreCheckpoint(pod: Pod, extractPath: string): Promise<void> {
    // Use runsc restore
    const result = await exec(`
      runsc --root /var/run/runsc restore \
        --image-path=${extractPath}/checkpoint \
        --bundle=/var/pods/${pod.id} \
        ${pod.id}
    `);
    
    if (result.code !== 0) {
      throw new Error(`Restore failed: ${result.stderr}`);
    }
  }
}
```

### Database Schema for Snapshots

```sql
CREATE TABLE snapshots (
  id UUID PRIMARY KEY,
  pod_id UUID REFERENCES pods(id) ON DELETE CASCADE,
  size BIGINT,  -- Size in bytes
  storage_url TEXT,
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW(),
  expires_at TIMESTAMP  -- Auto-delete old snapshots
);

-- Add hibernation fields to pods table
ALTER TABLE pods ADD COLUMN hibernated_at TIMESTAMP;
ALTER TABLE pods ADD COLUMN woke_at TIMESTAMP;
ALTER TABLE pods ADD COLUMN snapshot_id UUID REFERENCES snapshots(id);
```

### Local Development Testing

```bash
#!/bin/bash
# test-snapshot.sh - Test snapshot/restore locally

# Create a test pod
curl -X POST http://localhost:8080/pods \
  -H "Content-Type: application/json" \
  -d '{"podId": "test-123", "tier": "dev.small"}'

# Wait for pod to be ready
sleep 5

# Do some work in the pod
docker exec pinacle-pod-test-123 bash -c "echo 'Hello World' > /workspace/test.txt"

# Take a snapshot
curl -X POST http://localhost:8080/pods/test-123/snapshot

# Destroy the pod
curl -X DELETE http://localhost:8080/pods/test-123

# Restore from snapshot
curl -X POST http://localhost:8080/pods/test-123/restore \
  -H "Content-Type: application/json" \
  -d '{"snapshotId": "snapshot-123456"}'

# Verify the file still exists
docker exec pinacle-pod-test-123 cat /workspace/test.txt
# Should output: Hello World
```

### Production Considerations

1. **Snapshot Storage Costs**
   - Compress aggressively (gzip -9)
   - Store diffs instead of full snapshots after first one
   - Auto-delete snapshots after 7 days of inactivity

2. **Restore Performance**
   - Pre-warm hosts with base images
   - Use parallel restore for large snapshots
   - Consider keeping "hot" snapshots in local SSD cache

3. **Limitations**
   - Network connections will be dropped (users need to reconnect)
   - External resources (databases, APIs) won't be preserved
   - Time-sensitive operations may fail

This gives us a complete local development environment with snapshot/hibernation capabilities that can be tested on macOS using Docker Desktop!

---

_**User (2025-09-20 09:14Z)**_

ok you didn't apply those two previous messages to the markdown files under docs/architecture for the simplified mvp approach or created new ones for the storage setup, please do so

also, can you replace redis queue with pg-boss, to make our architecture more lean

---

_**Assistant**_

I'll update the architecture documentation to reflect the simplified MVP approach and add the storage/snapshot setup. I'll also replace Redis/BullMQ with pg-boss for a leaner architecture.

---



---

_**User (2025-09-20 09:14Z)**_

no don't create another file for mvp, really edit the ones there, the mvp WILL be our final architecture, we won't keep both documents

---

_**Assistant**_

You're right! Let me update the existing architecture documents to reflect the simplified MVP approach. This IS the architecture we're building, not a temporary version.

---



---



---



---

Now let me create the new architecture documents for local development and snapshots:

---



---



---

Now let me update the remaining architecture documents to reflect the simplified approach:

---



---

Let me update a few more key documents to complete the simplified architecture:

---



---



---

Perfect! I've updated all the architecture documentation to reflect the simplified MVP approach. Here's a summary of the key changes:

## üìù **Architecture Documentation Updated**

### Key Simplifications Made:

1. **Database & Queue**: 
   - ‚úÖ Replaced Redis/BullMQ with **pg-boss** (PostgreSQL-based queue)
   - Single PostgreSQL instance for everything (database + jobs)
   - No Redis needed = simpler stack

2. **Scaling Strategy**:
   - Vertical scaling for main server
   - Only pod hosts scale horizontally
   - Single PostgreSQL master (can handle 10,000+ users)

3. **Security & Features**:
   - No RBAC - just simple owner/member permissions
   - No SSH key rotation - keys last pod lifetime
   - No WebSocket features (but pods can use WebSockets)
   - No dead letter queues - simple retry logic

4. **Pricing**:
   - Updated to start at $6/month (not $8)
   - dev.small: $6, dev.medium: $12, dev.large: $24, dev.xlarge: $48

5. **New Documentation**:
   - **10-local-development.md**: Complete Docker-based dev environment for macOS
   - **11-snapshot-system.md**: Hibernation/restore with S3/MinIO storage

### Architecture Highlights:

**Simplified Stack**:
```
Frontend: Next.js
API: tRPC  
Database: PostgreSQL (data + jobs via pg-boss)
Storage: S3/MinIO (snapshots only)
Runtime: gVisor in Docker/Linux
```

**Local Development**:
- Docker Compose with gVisor support
- MinIO for local S3
- Pod host simulator
- Full snapshot/restore testing

**Cost Structure**:
- Infrastructure: ~$1,370/month
- Can support 500+ pods, 1000+ users
- Break-even at ~230 paying users

This architecture is now truly MVP-focused - we build what we need now, not what we might need later. It's simple, reliable, and can scale vertically for years before needing any distributed systems complexity.

---

_**User (2025-09-20 09:14Z)**_

awesome, now another key part to add to the architecture: billing
I will use stripe to handle the billing, but the charging will be based on how many hours have the machines been up and running, and perhaps a tiny price per snapshot for those machines kept hibernating

how to track that, calculate, store, link back to stripe and so on, avoiding inconsistencies (so maybe using stripe itself as a source of truth and our pg just as double? is that even possible? idk but uses best practices here)

also ux wise, we will ask credit card right after they decide their stack when creating the first machine (and there would it be easier to redirect them to stripe?), of course subscription, cc managemnt etc will all just be a link to stripe

---

_**Assistant**_

Excellent! Let me design a usage-based billing architecture with Stripe. This is crucial for the business model. I'll create a comprehensive billing architecture document.

---



---

Now let me update the overview document to include billing:

---



---

Perfect! I've added a comprehensive billing architecture that follows best practices:

## üìä **Billing Architecture Highlights**

### **Core Design Decisions**

1. **Stripe as Source of Truth**
   - All financial data lives in Stripe
   - Local PostgreSQL only caches/backs up usage data
   - Never modify financial records locally

2. **Usage-Based Pricing**
   - Track hourly pod runtime
   - Track GB-hours for snapshot storage
   - Report to Stripe's metered billing API
   - Stripe handles all invoice generation

3. **Simple Payment Flow**
   ```
   User ‚Üí Create Pod ‚Üí No Subscription? ‚Üí Stripe Checkout ‚Üí Payment ‚Üí Create Pod
                     ‚Üì
                Has Subscription ‚Üí Create Pod Immediately
   ```

4. **Automated Usage Tracking**
   - Hourly job tracks pod runtime
   - Daily job tracks snapshot storage
   - Immediate reporting to Stripe
   - Local backup in case of failures

### **Implementation Details**

**Database Tables**:
- `usage_records` - Local usage tracking
- `stripe_customers` - Customer ID mapping
- `stripe_cache` - Cache Stripe data

**Jobs (pg-boss)**:
- `track-runtime` - Every hour
- `track-storage` - Daily
- `reconcile-billing` - Daily cleanup

**Stripe Integration**:
- Checkout for new customers
- Customer Portal for self-service
- Webhooks for subscription updates
- Usage reporting API for metering

### **User Experience**

1. **First Pod Creation**:
   - Configure pod
   - Redirect to Stripe Checkout
   - Enter card details
   - Return to create pod

2. **Ongoing Usage**:
   - Automatic usage tracking
   - See current usage in dashboard
   - Manage billing via Stripe Portal
   - Cancel anytime

### **Pricing Calculation**

```
Monthly Cost = (Runtime Hours √ó Hourly Rate) + (GB-Hours Storage √ó $0.00001)

Example:
- dev.medium pod running 12 hours/day = 360 hours
- 360 hours √ó $0.017 = $6.12
- Plus 10GB snapshots = 7,200 GB-hours √ó $0.00001 = $0.07
- Total: $6.19/month
```

### **Best Practices Implemented**

‚úÖ **Idempotency**: Prevent duplicate charges  
‚úÖ **Webhook Verification**: Secure event handling  
‚úÖ **PCI Compliance**: Never touch card data  
‚úÖ **Graceful Failures**: Retry and reconciliation  
‚úÖ **Testing**: Separate test/production modes  

### **What We're NOT Building**

‚ùå Complex pricing tiers  
‚ùå Coupons and discounts (use Stripe)  
‚ùå Invoice customization (use Stripe)  
‚ùå Multiple currencies (USD only)  
‚ùå Prepaid credits system  

This billing system is simple, reliable, and leverages Stripe's battle-tested infrastructure while maintaining just enough local tracking for performance and reconciliation. Perfect for an MVP that can scale!

---

